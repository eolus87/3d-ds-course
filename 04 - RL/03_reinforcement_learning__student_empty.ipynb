{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-reinforcement_learning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science for the Automotive Industry: Third practical session - RL\n",
        "\n",
        "Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward (Wikipedia).\n",
        "\n",
        "In this session, we will grasp a feeling of a set of different reinforcement learning techniques:\n",
        "\n",
        "1) Thomson sampling\n",
        "\n",
        "2) Tabular methods\n",
        "\n",
        "3) Reinforcement learning based on NN\n",
        "\n",
        "<!-- References:\n",
        "\n",
        "https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
        "\n",
        "https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
        "\n",
        "https://medium.com/analytics-vidhya/rendering-openai-gym-environments-in-google-colab-9df4e7d6f99f \n",
        "-->\n",
        "\n",
        "Developed by Nicolas Gutierrez in February 2022"
      ],
      "metadata": {
        "id": "0bmLqwv7WTCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and importing all required packages"
      ],
      "metadata": {
        "id": "wklQQKK3dGCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing some extra packages we will need throughout the class."
      ],
      "metadata": {
        "id": "y-xRA2D-duM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Libraries for plotting and loading environments\n",
        "!pip3 install box2d-py\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2"
      ],
      "metadata": {
        "id": "zl0IByXbf1lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required libraries"
      ],
      "metadata": {
        "id": "19o3yjovd05m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import glob\n",
        "from collections import deque\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import beta\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "import gym\n",
        "\n",
        "# NNs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Utilities\n",
        "from colabgymrender.recorder import Recorder"
      ],
      "metadata": {
        "id": "QCNJ91DOctib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giving colab access to our google drive, so we can load the datasets and write results. A windows will pop up asking for permissions."
      ],
      "metadata": {
        "id": "14z9gbMtd31U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 1: Mount google drive\n",
        "\n",
        "# You will need two lines of code below\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "b8OnVsnd0zAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thomson Sampling\n",
        "\n",
        "Imagine you are working for Google, specifically for the advertising/marketing department. You have to give some advice to one of your customers that is a car seller. \n",
        "\n",
        "You are required to tell them what is the car that people like the most. This is not a survey and you are worried that decisions may be biased if you do that. You can use google platform to show announces one at a time and measure the success of the model by mesuring how often people clicks on it.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vEIBRzZWEs71ROACfsMi5z9mVS48sIK7)\n",
        "\n",
        "You have 10000 tries as a maximum but you will be rewarded if you use the minimum amount of them. \n",
        "\n",
        "What would you do?"
      ],
      "metadata": {
        "id": "wyAg6VHFcca0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset loading"
      ],
      "metadata": {
        "id": "o7f94aJJfz05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 2: Locate the file dataset.csv in your google drive\n",
        "\n",
        "# Use the required library and method and find the path\n",
        "list_of_files = \n",
        "#\n",
        "\n",
        "print(list_of_files)\n",
        "\n",
        "## If you did it correctly you will see a list with one file where the basename\n",
        "## is dataset.csv"
      ],
      "metadata": {
        "id": "P136r8UseJAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 10 ads that can be displayed to a user. Each time a user connects to the website, that makes a round or increases the index by one. At each index or round we can select just one ad to show to the user.\n",
        "\n",
        "\n",
        "If the result is 1 is that the user liked the ad and clicked it. Otherwise it is a 0."
      ],
      "metadata": {
        "id": "3zHntRUdC9dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 3: Load the file with pandas and print the first 5 files\n",
        "\n",
        "# Load the file in the folowing line\n",
        "ads_clicking = \n",
        "#\n",
        "\n",
        "# Show the 5 first lines in the following\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "I-HL8ZuS-vmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset exploration\n",
        "Let's have a look at the dataset. Even if in this situation this is not realistic, because in the hypothetic case described, we will never have accessed to all this data, we will plot the dataset in order to detect which one is the most successful ad."
      ],
      "metadata": {
        "id": "Dgmt1ARVVYqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 4: Show a bar plot with the number of clicks per ad\n",
        "\n",
        "# Display a bar plot where the horizontal axis will have the Ads and the vertical\n",
        "# axis will have the number of clicks\n",
        "\n",
        "#\n",
        "\n",
        "# Include title and names of the axis\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "## Which one is the most successful ad?"
      ],
      "metadata": {
        "id": "v-khdvHnI8c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model creation\n",
        "In this section, we will create a small model that will allow us detecting which is the most successful ad on the fly without having access to the whole data, just randomly sampling between all available options."
      ],
      "metadata": {
        "id": "rUibEYxhU3eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 5: Initialise accumulation variables\n",
        "\n",
        "# Initialise the following variables being:\n",
        "# N -> Number of rows in ads_clicking\n",
        "# d -> Number of columns in ads_clicking\n",
        "# numbers_of_rewards_1 -> a list of as many zeros as d\n",
        "# numbers of rewards_0 -> a list of as many zeros as d\n",
        "# total_reward -> an integer 0\n",
        "\n",
        "# Complete the following lines \n",
        "N, d = \n",
        "ads_selected = \n",
        "numbers_of_rewards_1 = \n",
        "numbers_of_rewards_0 = \n",
        "total_reward = \n",
        "#"
      ],
      "metadata": {
        "id": "w-39v-IyYygo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 6: Code the model\n",
        "\n",
        "for n in range(N):\n",
        "  # Initilization of variables per round\n",
        "  ad = 0\n",
        "  max_random = 0\n",
        "  \n",
        "  # Play a round to select one ad\n",
        "  for i in range(d):\n",
        "    # Investigate the betavariate distribution and calculate it based on initialised variables\n",
        "    random_beta = \n",
        "    #\n",
        "\n",
        "    if random_beta > max_random:\n",
        "      max_random = random_beta\n",
        "      ad = i\n",
        "  \n",
        "  # Evaluate the ad we selected \n",
        "  ads_selected.append(ad)\n",
        "  reward = ads_clicking.values[n, ad]\n",
        "  if reward == 1:\n",
        "    numbers_of_rewards_1[ad] += 1\n",
        "  else:\n",
        "    numbers_of_rewards_0[ad] += 1\n",
        "  total_reward += reward\n",
        "\n",
        "print(f\"Total reward is {total_reward}\")\n",
        "## If you did it right total reward should around 2600 (The final result has some randomness!!)\n",
        "## Careful if you run this cell twice in a row without resetting first the variables\n",
        "## in the previous cell!! "
      ],
      "metadata": {
        "id": "lxeMmDcnL2eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you understand the logic behind? "
      ],
      "metadata": {
        "id": "RX-NlZz9Hn2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot how our statistical agent is getting an understanding on what ad is the most succesful..."
      ],
      "metadata": {
        "id": "sTqbpDoITuxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 7: Create a bar plot with the selection of ads from the model\n",
        "\n",
        "# Complete the following lines (Include, title and names of x and y axis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "6g4rKHRiNSEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the model"
      ],
      "metadata": {
        "id": "5eLC32nYdRPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "def data_generation(numbers_of_rewards_1, numbers_of_rewards_0, n):\n",
        "  x = []\n",
        "  y = []\n",
        "  if n % 100 == 0:\n",
        "    for i in range(len(numbers_of_rewards_1)):\n",
        "      a = numbers_of_rewards_1\n",
        "      b = numbers_of_rewards_0\n",
        "      x.append(np.linspace(beta.ppf(0.01, a, b), beta.ppf(0.99, a, b), 100))\n",
        "      y.append(beta.pdf(x[-1], a, b))\n",
        "  return x, y\n",
        "\n",
        "def plot_distributions(x, y, n):\n",
        "  if n % 100 == 0:\n",
        "    plt.cla()\n",
        "    for i in range(len(x)):\n",
        "      pl.plot(x[i], y[i])\n",
        "    \n",
        "    pl.title(f\"Round {n}\")\n",
        "    pl.legend([f'Ad {j+1}' for j in range(len(x))])\n",
        "    display.display(pl.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    \n",
        "    time.sleep(0.05)"
      ],
      "metadata": {
        "id": "28wNnz0ZKXyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 8: Use the previous two methods to extrac x and y and plot them\n",
        "\n",
        "# Complete the following two lines\n",
        "x, y = \n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "aHDSeY7iKOLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's go into the statistical details of the implementation:\n",
        "\n",
        "The Beta distribution is best for representing a probabilistic distribution of probabilities: the case where we don't know what a probability is in advance, but we may have some reasonable guesses. Check this link for further details: [Intuition behind beta distribution](https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution)\n",
        "\n",
        "The beta distribution essentially __defines the probability that a Bernoulli experiment's success probability is p given the outcome of the experiment__.\n",
        "\n",
        "In the previous case, we start with α and β equal 1, where the beta distribution is a uniform distribution. In the first experiments it will just randomly sample the ads, but as soon as it starts hitting a success ad it will modify the distribution shape. According to the beta distribution, the chances of success of the Ad5 are around 0.27.\n"
      ],
      "metadata": {
        "id": "wLMWgIF7T5XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabular method\n",
        "\n",
        "In this exercise you will learn techniques based on Monte Carlo estimators to solve reinforcement learning problems in which you don't know the environmental behavior. We have to learn based on an episode by episode strategy and estimate the state-action values over many episodes to find an optimal/good policy. \n",
        "\n",
        "As an examble for this we consider the frozenlake environment provided by the gym API. The fozenlake environment is represented by a 4x4 grid consisting of a start grid , some hole grids and one goal grid. \n",
        "\n",
        "The agent can move, __up, down, right and left__. As further difficulty the grid is also slippery, meaning that an action might not lead to the desired direction. \n",
        "\n",
        "The rewards are set as follows: \n",
        "- 0 for each transition not entering the goal grid.\n",
        "- +1 for reaching the goal grid.\n",
        "\n",
        "<!-- Reference:\n",
        "https://www.deep-teaching.org/notebooks/reinforcement-learning/exercise-monte-carlo-frozenlake-gym\n",
        "-->\n"
      ],
      "metadata": {
        "id": "vJ5dp0eddO2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment creation and familiarization"
      ],
      "metadata": {
        "id": "ZW_q9YOYgAkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Instantiation of the environment\n",
        "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
        "\n",
        "# Printing the Action Space and the Observation space\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)"
      ],
      "metadata": {
        "id": "QiZ_KnVqfUrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "env.reset() #reset the environment the set agent to start state\n",
        "env.render() #display the agents current state on the grid"
      ],
      "metadata": {
        "id": "g1lGw9Tbfc93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 9: Execute the environment with random actions\n",
        "\n",
        "env.reset()\n",
        "for i in range(10):\n",
        "    # Investigate env class and action_space method to sample a random action\n",
        "    random_action =  \n",
        "    #\n",
        "    print(\"Action:\",random_action)\n",
        "    \n",
        "    # Investigate the env class to take the action\n",
        "    new_state, reward, done, info = \n",
        "    #\n",
        "    print(\"State:\", new_state)\n",
        "    \n",
        "    # new_state: new state after action (random_action) taken in current state \n",
        "    # reward: reward obtained after taken action (random_action) in current state and entering new state (new_state)\n",
        "    # done: bool flag, true if goal or hole is reached\n",
        "    # info: slippery probability, baseline is 1/3\n",
        "    \n",
        "    print(\"-----\")\n",
        "    env.render() # display current agent state\n",
        "    print(f\"Reward value: {reward}\\n\")\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "NB-YR54vlBPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The abbreviations stand for\n",
        "\n",
        "S: Start\n",
        "F: Frozen (safe)\n",
        "H: Hole\n",
        "G: Goal.\n",
        "The actions are numerated as left (0), right (1), down (2), up (3).\n",
        "\n",
        "A generic random walk until a terminal state is reached (done == True) is implemented below.\n",
        "\n",
        "Such a walk, until a terminal state is reached, represents one episode."
      ],
      "metadata": {
        "id": "7JW9FWHlfrJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model creation"
      ],
      "metadata": {
        "id": "AJYz2glogLF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 10: Initialise the variables for the model\n",
        "\n",
        "# Select a suitable num of episodes and epsilon\n",
        "# TIP: Start with episodes in the order of 10k and check what happens\n",
        "# epsilon will control the amount of exploration vs explotation, start\n",
        "# with small values <= 0.15\n",
        "num_episodes = \n",
        "epsilon = \n",
        "#\n",
        "\n",
        "# Initialise Q and n_s_a using numpy.\n",
        "# Q will hold as many q values as observation_space by action_space size\n",
        "# n_s_a will hold the times an action was taken based on a observation\n",
        "# Rest of variables will be empty \n",
        "Q = \n",
        "n_s_a = \n",
        "rList = \n",
        "success_rate = \n",
        "episode_length = \n",
        "#"
      ],
      "metadata": {
        "id": "nlS-HkE5lkg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 11: Build the model\n",
        "\n",
        "fig, ax = pl.subplots(ncols=2, figsize=np.array([2*6.4, 4.8]))\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    # Initialise variables\n",
        "    # state should reset the environment\n",
        "    # The rest of variables should be either False/emtpy or 0\n",
        "    state = \n",
        "    done = \n",
        "    results_list = \n",
        "    result_sum = \n",
        "    steps = \n",
        "    #\n",
        "\n",
        "    while not done:\n",
        "        # Look for a random number generator in numpy\n",
        "        random_number = \n",
        "        #\n",
        "\n",
        "        # In order to implement exploration vs explotation, code an if\n",
        "        # else clause that will sample an action randomly if random_number\n",
        "        # smaller than epsilon and will decide based on Q values otherwise.\n",
        "        # TIP: look for argmax function and keep in mind you are in a specific \n",
        "        # state currently\n",
        "        if random_number < epsilon:\n",
        "            action = \n",
        "        else:\n",
        "            action = \n",
        "        #\n",
        "        steps += 1\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        results_list.append((state, action))\n",
        "        result_sum += reward\n",
        "        state = new_state\n",
        "    \n",
        "    rList.append(result_sum)\n",
        "    if i>0 and i<100:\n",
        "      success_rate.append(sum(rList)/i)\n",
        "    elif i>=100:\n",
        "      success_rate.append(sum(rList[-100:])/100)\n",
        "    if i>0:\n",
        "      episode_length.append(steps)\n",
        "\n",
        "\n",
        "    # Complete the heart of the algorithm, the n_s_a and Q values.\n",
        "    for (state, action) in results_list:\n",
        "        # Increase the n_s_a indicated by 1.0\n",
        "        # alpha must be 1.0 divided by the previous n_s_a\n",
        "        # Q tries to reflect how successful was a movement in the context of\n",
        "        # achieving the objective of the game\n",
        "        n_s_a[state, action] += \n",
        "        alpha = \n",
        "        #\n",
        "        Q[state, action] += alpha * (result_sum - Q[state, action])\n",
        "\n",
        "    if i % 1000 == 0 and i is not 0:\n",
        "          plt.cla()\n",
        "          ax[0].plot(np.arange(0, i, 100), success_rate[::100], label='Success rate', color=u'#1f77b4', zorder=1)\n",
        "          ax[0].set_xlabel('Number of episodes')\n",
        "          ax[0].set_ylabel('Success rate')\n",
        "          ax[0].set_title(\"Success rate vs number of episodes\")\n",
        "         \n",
        "          ax[1].plot(np.arange(0, i, 100), episode_length[::100], label='Episode length', color='y', zorder=1)\n",
        "          ax[1].set_xlabel('Number of episodes')\n",
        "          ax[1].set_ylabel('Episode length')\n",
        "          ax[1].set_title(\"Episode length vs episodes\")\n",
        "\n",
        "          display.display(pl.gcf())\n",
        "          display.clear_output(wait=True)\n",
        "          time.sleep(0.05)\n",
        "\n",
        "print(\"Success rate: \" + str(sum(rList)/num_episodes))\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "a21ICbKbfsJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model testing"
      ],
      "metadata": {
        "id": "mFbYAbmwgOkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 12: Testing the trained model\n",
        "\n",
        "state = env.reset()\n",
        "for i in range(1000):\n",
        "    \n",
        "    # Use the same selection method than when training the model, but in this\n",
        "    # case you don't need exploration\n",
        "    action = \n",
        "    #\n",
        "\n",
        "    print(\"Action:\", action)\n",
        "    new_state, reward, done, info = env.step(action) #agent takes action (random_action)\n",
        "    state = new_state\n",
        "    \n",
        "    # new_state: new state after action (random_action) taken in current state \n",
        "    # reward: reward obtained after taken action (random_action) in current state and entering new state (new_state)\n",
        "    # done: bool flag, true if goal or hole is reached\n",
        "    # info: slippery probability, baseline is 1/3\n",
        "    \n",
        "    print(\"-----\")\n",
        "    env.render() # display current agent state\n",
        "    print(f\"Reward value: {reward}\\n\")\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# You will see the model cannot get to the objective every time. Do you know why?\n"
      ],
      "metadata": {
        "id": "vHD147Amgr9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN method"
      ],
      "metadata": {
        "id": "8zduXkU7dTiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Creation and familiarization"
      ],
      "metadata": {
        "id": "hkcC4_cwswik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 13: Select a folder for saving your video in google drive\n",
        "\n",
        "# Modify the following line\n",
        "video_path = '/content/drive/MyDrive/...'\n",
        "#"
      ],
      "metadata": {
        "id": "Zn449Qt5uvxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = Recorder(env, video_path)\n",
        "\n",
        "observation = env.reset()\n",
        "terminal = False\n",
        "while not terminal:\n",
        "  action = env.action_space.sample()\n",
        "  observation, reward, terminal, info = env.step(action)\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "TiiakoLlf9qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring/checking Tensorflow"
      ],
      "metadata": {
        "id": "K2lnPwUNs11w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "xENe9oPyZ8vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "r0rqQOY_ajaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Creation\n",
        "\n",
        "Finally, in this exercise we will try to develop a Model that is able to tackle more complicated problems. \n",
        "\n",
        "Previously we used MonteCarlo approach in tabular methods. Being a \"tabular\" space, there is a finite amount of states where our system can be, so we can actually visit them all and check how good the agent does.\n",
        "\n",
        "In this case, the state space is infinite/continous, so MonteCarlo approaches are not a good way of solving it. We will use a NN to \"map\" the state-reward space.\n",
        "\n",
        "Additionally, you will see how a class is coded in Python and how this is normally developed in a conventional research workflow."
      ],
      "metadata": {
        "id": "ucmcaV_YqSVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 14: Model class programming\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, env, mem_len, gamma, epsilon, epsilon_decay, lr, tau, batch_size):\n",
        "        self.env     = env\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon # Fraction of exploration\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = lr\n",
        "        self.tau = tau\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model        = self.create_model()\n",
        "        self.target_model = self.create_model()\n",
        "        \n",
        "        self.__initialize_memories(mem_len)\n",
        "        \n",
        "    # Do you know why the double underscore in the beginning?\n",
        "    def __initialize_memories(self, mem_len):\n",
        "        # Memories in this case are a buffer of the latest action-results. We \n",
        "        # need to maintain a certain size of the buffer to avoid overflow or\n",
        "        # too slow memory operations. This could be done with a simple list\n",
        "        # and some logic to remove new registries if required, but can you find\n",
        "        # a more suited variable type?\n",
        "        # TIP: check Python collections library!\n",
        "        \n",
        "        # Initialise the following class variables\n",
        "        self.memory_state = \n",
        "        self.memory_action = \n",
        "        self.memory_reward = \n",
        "        self.memory_new_state = \n",
        "        self.memory_done = \n",
        "        # \n",
        "\n",
        "    def create_model(self):\n",
        "        # Shape of the inputs of the model\n",
        "        state_shape  = self.env.observation_space.shape\n",
        "        \n",
        "        # Create a sequential tensorflow model with one dense layer, which\n",
        "        # input is state_shape[0] and activation is relu. The output layer\n",
        "        # is dense, with as many neurons as the action_space size and \n",
        "        # linear activation\n",
        "        model = \n",
        "        \n",
        "\n",
        "        #\n",
        "\n",
        "        # compile the model with loss MSE and adam optimizer, keep in mind\n",
        "        # the learning rate defined in the init function\n",
        "        \n",
        "\n",
        "        #\n",
        "\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        # Exploration vs exploitation\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "        \n",
        "        # Use the same logic for decision making as in the tabular methods, but \n",
        "        # in this case, instead of a Q table you will need to select from\n",
        "        # the prediction of the model\n",
        "        random_number = \n",
        "        if random_number < self.epsilon:\n",
        "            output = \n",
        "        else:\n",
        "            output = \n",
        "        #\n",
        "\n",
        "        return output\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        # Storing every variable in separate deques\n",
        "        self.memory_state.append(np.array(state)[0])\n",
        "        self.memory_action.append(action)\n",
        "        self.memory_reward.append(reward)\n",
        "        self.memory_new_state.append(np.array(new_state)[0])\n",
        "        self.memory_done.append(done)\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory_state) > self.batch_size: \n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(self.memory_state)), \n",
        "                                       size=self.batch_size)\n",
        "            \n",
        "            # Generate the subsampling of all variables\n",
        "            state = np.array(self.memory_state)[indices]\n",
        "            new_state = np.array(self.memory_new_state)[indices]\n",
        "            action = np.array(self.memory_action)[indices]\n",
        "            reward = np.array(self.memory_reward)[indices]\n",
        "            done = np.array(self.memory_done)[indices]\n",
        "            \n",
        "            # Use the target_model to predict state and new_state\n",
        "            target = self.target_model.predict(state)\n",
        "            q_inter = self.target_model.predict(new_state)\n",
        "            #\n",
        "\n",
        "            for i in range(len(target)):\n",
        "                if done[i]:\n",
        "                    target[i][action[i]] = reward[i]\n",
        "                else:\n",
        "                    q_future = max(q_inter[i])\n",
        "                    target[i][action[i]] = reward[i] + q_future * self.gamma\n",
        "            \n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "    def target_train(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)\n",
        "\n",
        "    def save_model(self, fn):\n",
        "        self.model.save(fn)\n",
        "\n",
        "def main():\n",
        "    env     = gym.make(\"MountainCar-v0\")\n",
        "    gamma   = 0.85\n",
        "    epsilon = 1\n",
        "    epsilon_decay = 0.999\n",
        "    lr = 0.005\n",
        "    tau = 0.125\n",
        "    \n",
        "    mem_len = 2048\n",
        "    \n",
        "    episodes  = 500\n",
        "    trial_len = 500\n",
        "\n",
        "    batch_size = 64\n",
        "\n",
        "    dqn_agent = DQN(env, mem_len, gamma, epsilon, epsilon_decay, lr, tau, batch_size)\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        tic = time.time()\n",
        "        print(f\"Episode: {episode}/{episodes}\")\n",
        "        \n",
        "        episode_reward = 0\n",
        "        cur_state = env.reset().reshape(1,2)\n",
        "        for step in range(trial_len):\n",
        "            # Model action\n",
        "            action = dqn_agent.act(cur_state)\n",
        "            \n",
        "            # Environment actioned\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # reward = reward if not done else -20\n",
        "            new_state = new_state.reshape(1,2)\n",
        "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
        "            \n",
        "            dqn_agent.replay()       # internally iterates default (prediction) model\n",
        "            if step % 4 == 0:\n",
        "                dqn_agent.target_train() # iterates target model\n",
        "\n",
        "            cur_state = new_state\n",
        "\n",
        "            episode_reward += reward\n",
        "            if done or reward != -1:\n",
        "                break\n",
        "        print(f\"\\t Current reward: {episode_reward}\")\n",
        "        print(f\"\\t Episode took {time.time()-tic:.2f}s\")\n",
        "        if step >= 199:\n",
        "            print(f\"\\t Failed to complete in episode {episode}\")\n",
        "        else:\n",
        "            now = datetime.now() # current date and time\n",
        "            date_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "            print(f\"\\t ------------- Completed in episode {episode}, step {step} ------------- \")\n",
        "            \n",
        "            # Find a suitable folder for saving your model\n",
        "            dqn_agent.save_model(f\"/content/drive/MyDrive/.../{date_time}_success_{episode}-{step}.model\")\n",
        "            #\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aND8DVwgxD2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {
        "id": "7fJ4mdULdW26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 15: Load the most promising model\n",
        "\n",
        "model = tf.keras.models.load_model(f\"/content/drive/MyDrive/...\")"
      ],
      "metadata": {
        "id": "mtMQEXCveiiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise \n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = Recorder(env, video_path)\n",
        "\n",
        "observation = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  prediction = model.predict(observation.reshape(1, 2))\n",
        "  action = np.argmax(prediction[0])\n",
        "  observation, reward, done, info = env.step(action)\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "kPR3Lkjc2-Sj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}