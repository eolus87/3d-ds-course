{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-machine_learning.ipynb","provenance":[{"file_id":"1hZNM3Gx0-0qLaHHVvbMQd_PQ-gaVO4wS","timestamp":1638389578793}],"mount_file_id":"142Wtp3S-0pYL31Yk0fSp34vIrRO9bqC3","authorship_tag":"ABX9TyNhvkS2CyPBH/nR5CBhPus0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Science for the Automotive Industry: First practical session - ML\n","\n","In this session, we will dive into an example of classic machine learning models, trying to extract the most out of a simple and small dataset using simple but powerful methods.\n","\n","We will explore a data set with car sales of different brands and models in USA. We will follow the order below:\n","1. Loading the dataset from google drive\n","2. Exploratory data analysis\n","3. Unsupervised learning\n","4. Supervised learning \n","\n","<!-- This data has been downloaded from [kaggle.com](https://www.kaggle.com/) and it can be found using [this link](https://www.kaggle.com/gagandeep16/car-sales) for car_sales,[this link](https://www.kaggle.com/smritisingh1997/car-salescsv) for car_sales_2 and [this link](https://www.kaggle.com/sachinsachin/car-sales?select=Car+Sales.xlsx) for car_sales_3. -->\n","\n","Developed by Nicolas Gutierrez in December 2021.\n"],"metadata":{"id":"sJGFf3ZWK8OV"}},{"cell_type":"markdown","source":["## Importing required libraries\n","It is a good practice loading the required libraries for the code at the start of it. Additionally, doing it this way you can have some hints about what the code below will do, just by checking the types of libraries imported."],"metadata":{"id":"fN6Kl4JPM5jo"}},{"cell_type":"code","metadata":{"id":"M2p3XlJUtR5p"},"source":["### Do not modify this cell, not an exercise\n","\n","# Files\n","import glob\n","\n","# Data loading and manipulation\n","import pandas as pd\n","\n","# Numeric operations\n","import numpy as np\n","from scipy.spatial.distance import cdist\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn import metrics, svm, tree\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n","from sklearn.cluster import KMeans\n","\n","# Representation\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","from matplotlib.gridspec import GridSpec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading the dataset from Google Drive\n","The best way to work with a dataset from google colab is loading it from the same folder where the notebook is stored using the following cell."],"metadata":{"id":"ki-O96BrMk2p"}},{"cell_type":"code","metadata":{"id":"qgH1gPJcsxAG"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once google drive is mounted and access is granded, we can use glob library to check the directory."],"metadata":{"id":"lCfIqXCGNRiv"}},{"cell_type":"code","metadata":{"id":"wl-MNzcNtUbj"},"source":["### Exercise 1: Locate the folder with the datasets in your google drive\n","\n","# Modify the following line\n","list_of_files = glob.glob('/content/drive/MyDrive/.../*')\n","#\n","\n","print(list_of_files)\n","## The result of this print should be 3 paths ending in */car_sales_3.csv, */car_sales_2.csv, */car_sales.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the previous list, we can use Pandas and load the csv \"Car_sales\" into memory."],"metadata":{"id":"cdoBA7LUNj2N"}},{"cell_type":"code","metadata":{"id":"EwHzFJ5at9FR"},"source":["### Exercise 2: Load the data from the file car_sales.csv using pandas, find the order to load a csv with pandas\n","\n","# Modify the following line\n","car_sales = \n","#\n","\n","print(f\"The number of rows of the file are: {len(car_sales)}\")\n","## The result should be 157"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exploratory Data Analysis\n","In this section, we will check the data to see what features/models/brands ... are included. Additionally we will check the distribution of values and get a feeling of statisical parameters, extremes and relationship of variables. "],"metadata":{"id":"6G8563pYNy9Y"}},{"cell_type":"code","source":["### Exercise 3: Find a way of showing the first or last rows of a pandas dataframe\n","\n","# Insert the order here\n","\n","#\n","\n","## The result should be a table in which the first column is Manufacturer"],"metadata":{"id":"QobdVUOENzaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 4: Find a method in your data frame car_sales that shows you a summary or a description of the content of the columns of the dataframe, Use the option of the method to describe all the data\n","\n","# Insert the order here\n","\n","#\n","\n","## The result should be a table with Manufacturer in the first column and then a some handy statistical values in the rows."],"metadata":{"id":"h4dRir63yyPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flBNw9hYvZlq"},"source":["### Exercise 5: Let's describe a bit more the data we have\n","\n","# Using your car sales dataframe complete the following lines\n","number_of_data_rows = \n","number_of_data_columns = \n","data_columns_as_a_list = \n","\n","manufacturers_list = \n","models_list = \n","vehicle_type_list = \n","\n","total_car_sales = \n","\n","most_successful_brand = \n","percentage_of_sales_of_most_successful_brand = \n","#\n","\n","## 157 rows and 16 columns\n","print(f\"The dataset has {number_of_data_rows} rows and {number_of_data_columns} columns\\n\")\n","## ['Manufacturer', 'Model', 'Sales_in_thousands', '__year_resale_value', ...\n","print(f\"Column names are {data_columns_as_a_list}\\n\")\n","## 30 and 156\n","print(f\"{len(manufacturers_list)} different brands and {len(models_list)} models are considered\\n\")\n","## ['Passenger' 'Car']\n","print(f\"Following vehicle types are included: {vehicle_type_list}\\n\")\n","## 8320698.0\n","print(f\"Total amount of sales: {total_car_sales}\\n\")\n","## Ford with 24.31% of the sales\n","print(f\"Most succesful brand (by sales): {most_successful_brand}, \"\n","f\"with {percentage_of_sales_of_most_successful_brand:.2f}% of the sales\")\n","\n","### What variables do you see as possible \"inputs\" and \"outputs\"?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsd4l_V4x5Yh"},"source":["### Exercise 6: Plot the car sales of every manufacturer so we can have a view of the most successful manufacturers\n","\n","# Don't use matplotlib for this, but directly from pandas, this should be done in one line\n","\n","#\n","plt.title(\"Sales by manufacturer\")\n","plt.ylabel(\"Sales [1000s of units]\")\n","\n","## The output should be a bar plot ordered from lowest to highest with the manufacturers in the x axis and the sales on the y axis."],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 7: Plot the car sales, but now based on the models.\n","\n","# Use the option figsize=(20, 5) to stretch the graph and see it better\n","\n","#\n","\n","plt.title(\"Sales by Model\")\n","plt.ylabel(\"Sales [1000s of units]\")\n","\n","## The output should be a bar plot ordered from lowest to highest with the manufacturers in the x axis and the sales in the y axis"],"metadata":{"id":"89fZ9Wi7s3yy"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt_Heog72mWl"},"source":["### Do not modify this cell, not an exercise\n","\n","# Let's plot histograms of all variables as this will help us understand how the values are distributed\n","def plot_histograms(vars, xlabels, title=None):\n","  ncols = len(vars)\n","  fig, ax = plt.subplots(ncols=ncols)\n","  plt.ylabel('Frequency [-]')\n","  if title:\n","    plt.suptitle(title)\n","  for i in range(len(vars)):\n","    average = car_sales[vars[i]].mean()\n","    median_value = car_sales[vars[i]].median()\n","    car_sales[vars[i]].plot.hist(ax=ax[i], sharey=True, figsize=(18,5))\n","    ax[i].set_title(\"Distribution of \\n\" + vars[i])\n","    ax[i].set_xlabel(xlabels[i])\n","    ax[i].axvline(x=average, color='red', zorder=1)\n","    ax[i].axvline(x=median_value, color='black', zorder=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 8: Use the function above to plot the 4 most relevant variables\n","\n","# Modify the following lines\n","variables_to_plot = \n","xlabel = \n","title = \n","plot_histograms(variables_to_plot, xlabel, title)\n","#\n","\n","### What do you thing are the most relevant variables? \n","### What do the red and black lines mean? \n","### What can you extract from them being close together or separated."],"metadata":{"id":"NPK0yOXUZzZI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2DNFCwK6w4A"},"source":["### Exercise 9: Use plot_histogram function to plot 'Physical characteristics of the cars\n","\n","# Include your lines here\n","\n","#"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature engineering\n","New features can be created as a combination of any other if required to enrich the model. For example in this data set we have width and length. Do you believe having the size will make a difference?"],"metadata":{"id":"PpZHL3IqR7vP"}},{"cell_type":"code","source":["### Exercise 10: Feature engineering\n","\n","# Create a column name called Size as a relevant combination of Width and Length\n","car_sales['Size'] = \n","#"],"metadata":{"id":"_ZIpbxaqRtxG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Correlation matrix\n","Pearson correlation value is a very powerful indicator of linear relationship \n","between two variables. It is very commonly used in Data Science \n","as knowing the relationship between different variables is extremely useful \n","for creating models. The highest the absolute value, the highest the \n","linear relationship. The Pearson coefficient value is bounded between -1 and 1."],"metadata":{"id":"lUh1C5-4SVpS"}},{"cell_type":"code","metadata":{"id":"vkhecG0n8jsA"},"source":["### Exercise 10: Pearson correlation matrix\n","\n","# Calculated the corr_matrix in the following line\n","corr_matrix = \n","#\n","\n","matfig = plt.figure(figsize=2*np.array([6.4, 4.8]))\n","plt.matshow(np.abs(corr_matrix), cmap=cm.RdYlGn, fignum=matfig.number)\n","plt.xticks(np.arange(0, len(corr_matrix.columns)), corr_matrix.columns.to_list(), rotation=90)\n","plt.yticks(np.arange(0, len(corr_matrix.columns)), corr_matrix.columns.to_list())\n","plt.colorbar()\n","\n","## Check the output of this cell, is there anything that calls your attention?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Unsupervised learning\n","In this section we will try to check if we can find any hidden relationship in the data by means of clustering. We will use [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and the famous [elbow method](https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/) to detect the optimal number of clusters.\n","\n","We will try to look for an example like [this](https://datatofish.com/k-means-clustering-python/), where clusters are clearly defined. "],"metadata":{"id":"dt4T57J1dlni"}},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","def plot_couple_of_variables(dataset, x_name, y_name):\n","  x_variable = dataset[x_name]\n","  y_variable = dataset[y_name]\n","\n","  fig = plt.figure()\n","  gs = GridSpec(4, 4)\n","\n","  ax_scatter = fig.add_subplot(gs[1:4, 0:3])\n","  ax_hist_x = fig.add_subplot(gs[0,0:3])\n","  ax_hist_y = fig.add_subplot(gs[1:4, 3])\n","\n","  ax_scatter.scatter(x_variable, y_variable)\n","  ax_scatter.set_xlabel(x_name)\n","  ax_scatter.set_ylabel(y_name)\n","\n","  ax_hist_x.hist(x_variable)\n","  ax_hist_y.hist(y_variable, orientation = 'horizontal')"],"metadata":{"id":"mepl7mbV9-jy"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9h5iJ3JHL1Pz"},"source":["### Exercise 11: Select two relevant variables from the car_sales dataset\n","\n","# Modify the following lines\n","x_name = \"\"\n","y_name = \"\"\n","#\n","\n","plot_couple_of_variables(car_sales, x_name, y_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dbnn5rKOSNBh"},"source":["### Exercise 12: Prepare the dataset\n","\n","# First, remove the nan from the data set in the following line\n","car_sales_nandropped = \n","#\n","\n","X = np.array(list(zip(car_sales_nandropped[x_name], \n","                      car_sales_nandropped[y_name])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 13: Fit a KMeans as a unsupervised learning method\n","\n","# Modify the number of clusters and check the results\n","number_of_clusters = \n","\n","kmeanModel = \n","#"],"metadata":{"id":"Fkex0PojBo1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 14: Predict the data in X using kmeanModel object\n","\n","# Modify the following line\n","results = \n","#"],"metadata":{"id":"mTs77qkgB4e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXOxoaL2S_jy"},"source":["### Do not modify this cell, not an exercise\n","print(results)\n","for i in range(number_of_clusters):\n","  points_belonging_to_i_cluster = results == i\n","  plt.scatter(car_sales_nandropped[x_name].to_numpy()[points_belonging_to_i_cluster], \n","              car_sales_nandropped[y_name].to_numpy()[points_belonging_to_i_cluster], \n","              label=f\"Cluster {i}\")\n","plt.legend()\n","plt.xlabel(x_name)\n","plt.ylabel(y_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you may have seen already, the number of clusters to use is not very clear and seems a bit subjective. Data Science tries to not rely on opinions and for this case what is normally used is the elbow method. "],"metadata":{"id":"QkyjJPGuCrB2"}},{"cell_type":"code","metadata":{"id":"cSCyLyDeHzuu"},"source":["### Exercise 15: Create a function to plot a graph for the elbow method\n","\n","def elbow_method(X):\n","  distortions = []\n","  K = range(1, 10)\n","  \n","  for k in K:\n","      # Complete the following lines to define the KMeans method knowing that \n","      # in every iteration we want to use a k number of clusters and then fit\n","      # to X\n","      kmeanModel = \n","      \n","      #\n","\n","      ## Distortions are calculated as the distance between every point and the \n","      ## cluster centers divided by the number of points in X\n","      distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n","                                          'euclidean'), axis=1)) / X.shape[0])\n","    \n","  plt.plot(K, distortions, 'bx-')\n","  plt.xlabel('Values of K')\n","  plt.ylabel('Distortion')\n","  plt.title('The Elbow Method using Distortion')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 16: Use the previous function to plot a figure with the elbow method\n","\n","# Fill the following line\n","\n","#"],"metadata":{"id":"4wFekoH3fcIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Supervised Learning I: Price prediction\n","\n","In this section we will try to create a model that help us understanding the behaviour of customers based on previous data.\n","\n","Wouldn't be cool if we could predict how much would someone pay for a car with certain characteristics?"],"metadata":{"id":"2ILYviF6g1Kl"}},{"cell_type":"markdown","source":["### Features/labels selection\n","Let's select first which ones will be the inputs (features) and outputs (labels) of our supervised learning problem."],"metadata":{"id":"6s91-WfOikuW"}},{"cell_type":"code","source":["### Exercise 17: Prepare the data to be used in a supervised learning method\n","\n","# Select the input and output variable names, input is a list, output is a single one\n","input_variables = ['']\n","output_variable = ''\n","#\n","\n","# Preparation of the data\n","input_variables.append(output_variable)\n","\n","X = car_sales[input_variables].dropna(axis=0)\n","\n","y = X[output_variable]\n","X.drop([output_variable], axis=1, inplace=True)\n","\n","# Showing the result to the user\n","print(f\"{len(X.columns)} input variables: {X.columns}\")\n","print(f\"Output variable is: {y.name}\")"],"metadata":{"id":"rFwJzjJSi8w5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalization/Standarization of variables\n","Data should be normalized or standarised before getting into any machine learning model. The purpose of this is to avoid biasing the model towards the higher magnitude variables."],"metadata":{"id":"OxH8bbEskASu"}},{"cell_type":"code","source":["### Exercise 18: Normalization\n","\n","# Find a scaler to normalise the data based on min and max (Have a look at sklearn.preprocessing)\n","scaler_x = \n","scaler_y = \n","#\n","\n","scaler_x.fit(X)\n","scaler_y.fit(y.to_numpy().reshape(-1,1))\n","\n","X_norm = scaler_x.transform(X)\n","y_norm = scaler_y.transform(y.to_numpy().reshape(-1,1))"],"metadata":{"id":"F3nX9jdDhga2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 19: Standarization\n","\n","# Find a scaler to standarise the data based on average and standard deviation\n","standard_x = \n","standard_y = \n","#\n","\n","standard_x.fit(X)\n","standard_y.fit(y.to_numpy().reshape(-1,1))\n","\n","X_standard = standard_x.transform(X)\n","y_standard = standard_y.transform(y.to_numpy().reshape(-1,1))"],"metadata":{"id":"8QOGs6TGHlO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train/test split\n","Next step we will do is plitting the data into train and test datasets. This is done to evaluate the models fairly. The models will be trained with the train data and they normally will perform very well on this one. The test dataset is used to evaluate the performance of the model on data it has never seen before or how good it generalises."],"metadata":{"id":"kd4jpnNOkH9I"}},{"cell_type":"code","source":["### Exercise 20: Find a method to split into train and test\n","\n","# Modify the method name in the following lines\n","# Non processed data\n","X_np_train, X_np_test, y_np_train, y_np_test = \\\n","METHODNAME(X.to_numpy(), y.to_numpy().reshape(-1,1), test_size=0.3, random_state=1)\n","#\n","\n","# Modify the method name in the following lines\n","# Normalization\n","X_norm_train, X_norm_test, y_norm_train, y_norm_test = \\\n","METHODNAME(X_norm, y_norm, test_size=0.3, random_state=1)\n","#\n","\n","# Modify the method name in the following lines\n","# Standarization\n","X_standard_train, X_standard_test, y_standard_train, y_standard_test = \\\n","METHODNAME(X_standard, y_standard, test_size=0.3, random_state=1)\n","#"],"metadata":{"id":"W3yngabzeYee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fitting Models"],"metadata":{"id":"OUe5yJE4pZka"}},{"cell_type":"code","source":["### Exercise 21: Fit models to the data\n","\n","def fit_models(features_train, labels_train):\n","  # Look for a linear regressor\n","  reg = \n","  #\n","  reg.fit(features_train, labels_train)\n","  \n","  # Look for Support vector machines, use the 'linear' kernel\n","  svm_linear = \n","  #\n","  svm_linear.fit(features_train, labels_train.ravel())\n","  \n","\n","  # Look for Support vector machines, use the 'poly' kernel\n","  svm_poly = \n","  #\n","  svm_poly.fit(features_train, labels_train.ravel())\n","  \n","\n","  # Look for Support vector machines, use the 'rbf' kernel\n","  svm_rbf = \n","  #\n","  svm_rbf.fit(features_train, labels_train.ravel())\n","  \n","  # Look for regressor based on decision trees\n","  clf = \n","  #\n","  clf.fit(features_train, labels_train.ravel())\n","  \n","  return reg, svm_linear, svm_poly, svm_rbf, clf"],"metadata":{"id":"-IRTsetjee--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Modify this cell if you want to use features normalised or standarised\n","\n","features_train = X_np_train\n","labels_train = y_np_train\n","features_test = X_np_test\n","labels_test = y_np_test\n","\n","reg, svm_linear, svm_poly, svm_rbf, clf = fit_models(features_train, labels_train)"],"metadata":{"id":"Bgc1xj09toCg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation of the models"],"metadata":{"id":"TYdOAkqVrZD7"}},{"cell_type":"code","source":["### Exercise 21: Evaluate the models\n","\n","def evaluation_of_methods(methods_vector, methods_name, \n","                          features_train, features_test, \n","                          labels_train, labels_test):\n","  evaluation = pd.DataFrame(columns=['method', 'r2_train', 'r2_test', \n","                                     'MSE_train', 'MSE_test', \n","                                     'MAE_train', 'MAE_test'])\n","  for i in range(len(methods_vector)):\n","    print(f\"method name:{methods_name[i]}\")\n","    results_dict = dict()\n","    results_dict['method'] = methods_name[i]\n","    \n","    y_hat_train = methods_vector[i].predict(features_train)\n","    y_hat_test = methods_vector[i].predict(features_test)\n","\n","    # Look for suitable methods to calculate the following metrics\n","    results_dict['r2_train'] = XXXX(labels_train, y_hat_train)\n","    results_dict['r2_test'] = XXXX(labels_test, y_hat_test)\n","\n","    results_dict['MSE_train'] = XXXX(labels_train, y_hat_train)\n","    results_dict['MSE_test'] = XXXX(labels_test, y_hat_test)\n","\n","    results_dict['MAE_train'] = XXXX(labels_train, y_hat_train)\n","    results_dict['MAE_test'] = XXXX(labels_test, y_hat_test)\n","    #\n","\n","    method_results = pd.DataFrame.from_dict([results_dict])\n","    evaluation = evaluation.append(method_results, ignore_index = True)\n","    \n","  return evaluation"],"metadata":{"id":"Y3zUdF5HrBh3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","methods_vector = [reg, svm_linear, svm_poly, svm_rbf, clf]\n","methods_name = ['Linear Regressor', 'SVM Linear', 'SVM Poly', 'SVM RBF', 'Decision Tree']\n","evaluation = evaluation_of_methods(methods_vector, methods_name, \n","                                   features_train, features_test,\n","                                   labels_train, labels_test)"],"metadata":{"id":"d-iRqVudueLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","evaluation"],"metadata":{"id":"wPgvUgpzsExZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","def plotting_methods(methods_vector, features_test, labels_test):\n","  nrows = len(methods_vector)\n","  fig, ax = plt.subplots(ncols=2, \n","                        nrows=nrows, \n","                        figsize=2.5*np.array([6.4, 4.8]), \n","                        sharex='col')\n","  for i in range(nrows):\n","    # Prediction\n","    y_hat = methods_vector[i].predict(features_test).ravel()\n","    # Prediction vs real\n","    ax[i, 0].scatter(labels_test, y_hat)\n","    ax[i, 0].scatter(labels_test, labels_test)\n","    ax[i, 0].set_ylabel(f'Predictions {methods_name[i]}')\n","    ax[i, 0].set_xlabel('Real values')\n","    ax[i, 1].hist(labels_test.ravel()-y_hat)\n","    ax[i, 1].set_xlabel('Error')"],"metadata":{"id":"RsnBw_K5Mezf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","plotting_methods(methods_vector, features_test, labels_test)\n","\n","## Have you tried launching the fitting of the models with normalised or standarised features?"],"metadata":{"id":"lYP5lHQoyT_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Supervised Learning II: Sales Prediction\n","\n","The marketing department of a new brand has contacted our group of data science with interest in sales prediction. Based on the dataset we have, we will try to create a model that can predict the number of sales of a model given its characteristics."],"metadata":{"id":"yZih-wwM-r7x"}},{"cell_type":"markdown","source":["### Features/labels preparation\n","The manufacturer is probably a thing that will be very interesting in this study, so we will need to find a way of including that information into a model. The problem is that the manufacturer is a string. How can we include a string of this type into a data science model?\n","\n","One of the techniques is one hot encoding and consists of creating as many columns as manufacturers and including with a 1 when the row belongs to that manufacturer and a 0 otherwise."],"metadata":{"id":"Qe74eJDDvnfu"}},{"cell_type":"code","source":["### Exercise 21: One hot encoding for the manufacturers\n","\n","# Create the manufacturers one hot encoding using a method from pandas \n","manufacturer_one_hot = \n","#\n","\n","## Quick check: The result should be 157\n","print(np.sum(manufacturer_one_hot.sum(axis=1)))\n","print(len(manufacturer_one_hot))"],"metadata":{"id":"N5yDaS_P_kU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 22: Clean the data frame by removing some columns\n","### Remove columns ['Manufacturer', 'Model', '__year_resale_value', 'Vehicle_type', 'Latest_Launch']\n","\n","# Find a way in pandas to remove columns from a dataframe\n","car_sales_for_sales = \n","#\n","\n","## Quick check with the remaining columns\n","print(car_sales_for_sales.columns)"],"metadata":{"id":"cjT5zCU2TfR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 23: Include the manufacturers into the car_sales_for_sales data frame\n","\n","# Find a way in pandas to include this new columns into the dataframe\n","\n","#\n","\n","## Quick check, you should see now columns with the name of the manufacturer and the one hot encoding\n","car_sales_for_sales.head(3)"],"metadata":{"id":"qru4_cnjUJc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 24: Remove any entry that may have a not a number\n","\n","# Find a way in pandas to remove registries with not a number\n","car_sales_ii_ready = \n","#"],"metadata":{"id":"TScGE77_VjdH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Features/labels selection"],"metadata":{"id":"4Lgv_5dqvuYy"}},{"cell_type":"code","source":["### Exercise 25: Select the inputs and outputs for this problem\n","\n","# Include relevant input variables as list (don't forget to include the manufacturer names)\n","input_variables = []\n","#\n","\n","# Include the output of the output variable\n","output_variable = \n","#"],"metadata":{"id":"vaFBLXZGV9iP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","input_variables.append(output_variable)\n","\n","X_ii = car_sales_ii_ready[input_variables].dropna(axis=0)\n","\n","y_ii = X_ii[output_variable]\n","X_ii.drop([output_variable], axis=1, inplace=True)\n","\n","## Quick check the data\n","print(f\"{len(X_ii.columns)} input variables: {X_ii.columns}\")\n","print(f\"Output variable is: {y_ii.name}\")"],"metadata":{"id":"UQcjcaJeWgxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Standarization of variables"],"metadata":{"id":"sVl_tovEU7Uh"}},{"cell_type":"code","source":["### Exercise 26: Standardise the data\n","\n","# Instatiate the standariser\n","standard_x_ii = \n","standard_y_ii = \n","#\n","\n","standard_x_ii.fit(X_ii)\n","standard_y_ii.fit(y_ii.to_numpy().reshape(-1,1))\n","X_standard_ii = standard_x_ii.transform(X_ii)\n","y_standard_ii = standard_y_ii.transform(y_ii.to_numpy().reshape(-1,1))"],"metadata":{"id":"Fj3kmBeaWkgU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train/test split"],"metadata":{"id":"BVINzVujVNVJ"}},{"cell_type":"code","source":["### Exercise 27: Split the dataset into training/test\n","\n","# Modify the method name in the following lines\n","X_standard_train_ii, X_standard_test_ii, y_standard_train_ii, y_standard_test_ii = \\\n","METHOD_NAME(X_standard_ii, y_standard_ii, test_size=0.3, random_state=1)\n","#"],"metadata":{"id":"83Yy-hKNWo5u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PCA\n","It is a quite famous dimensionality reduction technique that allows us reducing the size of the problem (compressing the information of the features) losing information. The more PCA components we take into account the less information we lose. \n","\n","PCA is useful when we have problems with many features and not a huge amount of data."],"metadata":{"id":"lz_rW2qCv7D-"}},{"cell_type":"code","source":["### Exercise 27: Plot the PCA explained variace ratio as a function of the number of variables\n","\n","var_exp = []\n","for i in range(1, len(input_variables)):\n","  # Find the pca method and use i as number of components and fit it to X_standard_train_ii\n","  pca = \n","\n","  #\n","\n","  # Find the variance ratio and append to the var_exp list the sum.\n","  var_exp.append()\n","  #\n","\n","plt.figure()\n","plt.plot(np.arange(1,len(input_variables)), var_exp)"],"metadata":{"id":"gakmYrC3d5Pt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Exercise 28: Select a suitable number of components and fit the PCA\n","\n","# Based on the previous results select a number of components and fit the pca to X_standard_train_ii\n","pca = \n","\n","#\n","\n","features_train = pca.transform(X_standard_train_ii)\n","labels_train = y_standard_train_ii\n","features_test = pca.transform(X_standard_test_ii)\n","labels_test = y_standard_test_ii"],"metadata":{"id":"ThUHMd-eWtUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fitting models"],"metadata":{"id":"B8W6TOrnv9WD"}},{"cell_type":"code","source":["### Exercise 29: Fit models to the data\n","\n","# Check previous section and fit a linear regressor, support vector machines and decission trees to the data\n","\n","#"],"metadata":{"id":"JjUsHdFoWwJm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation of models"],"metadata":{"id":"ZMby-gHpwAKV"}},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","methods_vector = [reg, svm_linear, svm_poly, svm_rbf, clf]\n","methods_name = ['Linear Regressor', 'SVM Linear', 'SVM Poly', 'SVM RBF', 'Decision Tree']\n","evaluation = evaluation_of_methods(methods_vector, methods_name, \n","                                   features_train, features_test,\n","                                   labels_train, labels_test)"],"metadata":{"id":"6pOzD1k0W0HC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","evaluation"],"metadata":{"id":"9AoGPbEpY7UH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","plotting_methods(methods_vector, features_test, labels_test)"],"metadata":{"id":"0UXHCdyLW5z5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Do not modify this cell, not an exercise\n","\n","real_errors = standard_y_ii.inverse_transform((labels_test.ravel() - svm_poly.predict(features_test).ravel()).reshape(-1,1))\n","print(f\"Real errors {real_errors.ravel()}\")\n","print(f\"Real error average  {np.mean(real_errors)}\")\n","print(f\"Real error Standar deviation  {np.std(real_errors)}\")"],"metadata":{"id":"l3UnFzRFY1Ni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusions and take aways\n","- Even with a small dataset, many insights can be unveiled.\n","- The bigger and the richer the data set, the greater the findings.\n","- Many times, just with a simple exploratory data analysis give us a lot of information.\n","- With unsupervised learning we can detect hidden patterns in the data.\n","- With supervised learning we can create models to predict variables for new entries."],"metadata":{"id":"6dPOgsJewEa-"}}]}